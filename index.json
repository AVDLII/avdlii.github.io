[{"content":"I would like to share my updated homelab. I pulled the trigger and bought an old/used mini PC. The specs are decent enough for my own use-case and I think for its performance, it is worth the price.\nFor starters, I decided to use Proxmox as my main homelab OS. At first I thought I would just test it out and see what happens but the more I tinker with it, the more I am liking it so I stuck with Proxmox. I also like the added benefit of spinning up a new VM in the web interface. This is now my updated homelab diagram. It is still as simple as the previous one but I am working on making it more like into more of a production-grade environment: Nothing has changed from my previous setup. I am still running Pi-hole as my ad-blocker and DNS coupled with Nginx Proxy Manager for reverse proxy and resolving hostnames (I will also work on adding SSL certificates in my homelab with this). I am still hosting my media server Jellyfin for our family entertainment needs. And of course, Grafana and Prometheus for my monitoring stack. The only thing that changed is that I am now hosting separate instances of node_exporter and connecting them all to a centralized Prometheus server so that I can monitor servers remotely outside Prometheus\u0026rsquo; docker network. And also, this is where I will spin up my kubernetes testing lab. This week has been very productive so far and I am learning a lot!\n","permalink":"https://avdlii.xyz/posts/homelab-update-2/","summary":"I would like to share my updated homelab. I pulled the trigger and bought an old/used mini PC. The specs are decent enough for my own use-case and I think for its performance, it is worth the price.\nFor starters, I decided to use Proxmox as my main homelab OS. At first I thought I would just test it out and see what happens but the more I tinker with it, the more I am liking it so I stuck with Proxmox.","title":"Homelab Update: Proxmox"},{"content":"This past week, all I did was update and refine my homelab environment. I am doing all the setup on my old Ubuntu PC and deploying my applications on docker containers. It was a lot of funand I am learning a lot.\nFor those who don\u0026rsquo;t know me, I love watching (and rewatching) movies and TV shows. Especially those that left a mark in my brain. So when I found out that I can self-host my own media server, I was amazed. And with that, I fell into the rabbithole of self-hosting and homelabbing.\nI want to share my homelab diagram. This is just the basic setup and I will update it as I go. I might even buy an old mini PC to use as a server. But now, this will do:\nPi-hole PiHole is a network-wide adblocking application that acts as my DNS for my other applications.\nNginx Proxy Manager Nginx Proxy Manager is my reverse proxy server for redirecting the server requests on my private network to their respective IP/ports.\nJellyfin My self-hosted media server. All the users in our home network can access my Jellyfin server to watch movies and shows.\nVaultwarden My password manager for all my sites. I think this is much safer since this is privately-hosted.\nPrometheus, Grafana, CAdvisor As I discussed in my previous post, this monitoring stack provides data that is converted into a visualized graphs for monitoring my containers\u0026rsquo; resources. As of now, I am deploying all of this in a single docker compose file. But as this setup becomes more static, I am gonna use Portainer for my container management.\n","permalink":"https://avdlii.xyz/posts/homelab-update/","summary":"This past week, all I did was update and refine my homelab environment. I am doing all the setup on my old Ubuntu PC and deploying my applications on docker containers. It was a lot of funand I am learning a lot.\nFor those who don\u0026rsquo;t know me, I love watching (and rewatching) movies and TV shows. Especially those that left a mark in my brain. So when I found out that I can self-host my own media server, I was amazed.","title":"Homelab Self-hosting Update"},{"content":"It\u0026rsquo;s been a while since I heard about Prometheus and Grafana.\nIt is an observability stack that can be used to monitor container resources.\nPrometheus is for gathering and consolidating the resources data scraped from the containers and Grafana is for data visualization.\nI decided to try it out for myself and use it for my future homelab projects!\nBelow is the docker-compose.yml file I used\n--- volumes: prometheus-data: driver: local grafana-data: driver: local services: prometheus: image: docker.io/prom/prometheus:v2.53.0 container_name: prometheus ports: - 9090:9090 command: \u0026#34;--config.file=/etc/prometheus/prometheus.yaml\u0026#34; volumes: - ./config/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro - prometheus-data:/prometheus restart: unless-stopped grafana: image: docker.io/grafana/grafana-oss:11.1.0 container_name: grafana ports: - 3000:3000 volumes: - grafana-data:/var/lib/grafana restart: unless-stopped node_exporter: image: quay.io/prometheus/node-exporter:v1.1.0 container_name: node_exporter command: \u0026#34;--path.rootfs=/host\u0026#34; pid: host restart: unless-stopped volumes: - /:/host:ro,rslave cadvisor: image: gcr.io/cadvisor/cadvisor:v0.49.1 container_name: cadvisor ports: - 8080:8080 volumes: - /:/rootfs:ro - /run:/run:ro - /sys:/sys:ro - /var/lib/docker:/var/lib/docker:ro - /dev/disk:/dev/disk:ro devices: - /dev/kmsg privileged: true restart: unless-stopped Breakdown The first service is for Prometheus. I used version 2.53 and created volumes for the configuration file and the main prometheus data persistency:\nprometheus: image: docker.io/prom/prometheus:v2.53.0 container_name: prometheus ports: - 9090:9090 command: \u0026#34;--config.file=/etc/prometheus/prometheus.yaml\u0026#34; volumes: - ./config/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro - prometheus-data:/prometheus restart: unless-stopped Next is Grafana, our data visualizer using the data gathered by Prometheus\ngrafana: image: docker.io/grafana/grafana-oss:11.1.0 container_name: grafana ports: - 3000:3000 volumes: - grafana-data:/var/lib/grafana restart: unless-stopped Then there\u0026rsquo;s Node Exporter, which Prometheus use to gather host resources metrics\nnode_exporter: image: quay.io/prometheus/node-exporter:v1.1.0 container_name: node_exporter command: \u0026#34;--path.rootfs=/host\u0026#34; pid: host restart: unless-stopped volumes: - /:/host:ro,rslave And lastly, CAdvisor, which Prometheus use to gather resource metrics of our docker containers\ncadvisor: image: gcr.io/cadvisor/cadvisor:v0.49.1 container_name: cadvisor ports: - 8080:8080 volumes: - /:/rootfs:ro - /run:/run:ro - /sys:/sys:ro - /var/lib/docker:/var/lib/docker:ro - /dev/disk:/dev/disk:ro devices: - /dev/kmsg privileged: true restart: unless-stopped Apparently, CAdvisor and Node Exporter needs access to the root directory, so do not deploy it on an unsecured environment.\nThis is quite nice!\nThis dashboard is from the cadvisor data\nThis dashboard is from the node-exporter data. That\u0026rsquo;s a lot of metrics!\nI can\u0026rsquo;t wait to deploy these along with my self-hosted homelab servers.\n","permalink":"https://avdlii.xyz/posts/prometheus-grafana-test/","summary":"It\u0026rsquo;s been a while since I heard about Prometheus and Grafana.\nIt is an observability stack that can be used to monitor container resources.\nPrometheus is for gathering and consolidating the resources data scraped from the containers and Grafana is for data visualization.\nI decided to try it out for myself and use it for my future homelab projects!\nBelow is the docker-compose.yml file I used\n--- volumes: prometheus-data: driver: local grafana-data: driver: local services: prometheus: image: docker.","title":"Prometheus Grafana Simulation"},{"content":"I\u0026rsquo;m considering transforming my Windows PC into a homelab. Currently, it runs both Windows and Ubuntu in dual-boot mode, allowing me to ssh into Ubuntu and create virtual machines using virsh for testing and simulation.\nAs I research more, I discovered numerous services and applications that I could host on this older PC.\nTo start, since I primarily use this PC for virtual machines, why not experiment with Proxmox? Additionally, as for watching TV series and movies, hosting my own Jellyfin server sounds exciting. Concerned about managing passwords across different sites? I\u0026rsquo;ll set up my own Bitwarden server.\nThe possibilities are just too many to ignore so I think I\u0026rsquo;m gonna do it.\n","permalink":"https://avdlii.xyz/posts/homelab-ideas/","summary":"I\u0026rsquo;m considering transforming my Windows PC into a homelab. Currently, it runs both Windows and Ubuntu in dual-boot mode, allowing me to ssh into Ubuntu and create virtual machines using virsh for testing and simulation.\nAs I research more, I discovered numerous services and applications that I could host on this older PC.\nTo start, since I primarily use this PC for virtual machines, why not experiment with Proxmox? Additionally, as for watching TV series and movies, hosting my own Jellyfin server sounds exciting.","title":"Homelab Ideas"},{"content":"As said in my earlier post, my python application has a PostgreSQL backend running on a separate VM. But in order to fully appreciate Docker, I decided to try out PostgreSQL on Docker.\nFirst, let\u0026rsquo;s pull the official PostgreSQL image for alpine\n$ docker pull postgres:15-alpine Then run the image and include the environment variable POSTGRES_PASSWORD for now. We will also add a volume for data persistence.\n$ docker run -itd --rm --name psql \\ \u0026gt; -e POSTGRES_PASSWORD=******** \\ \u0026gt; -v ./dbdata:/var/lib/postgresql/data \\ \u0026gt; postgres:15-alpine Now, let\u0026rsquo;s look at the container ip address so we can access it using psql.\n$ docker inspect bridge # output truncated ... \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;6c414502dbe431761f838726180b073b579a855c178a996ff57e9b8510a33f4d\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;psql\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;562ba21b5b1c1cf18e535887c0c77366f6f087c4f2db6b154e0ccdb7f970eede\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;22:81:bd:2d:1c:52\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.17.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } ... To access the container using psql\n$ psql -h 172.17.0.2 -d postgres -U postgres Password for user postgres: psql (14.12 (Ubuntu 14.12-0ubuntu0.22.04.1), server 15.7) WARNING: psql major version 14, server major version 15. Some psql features might not work. Type \u0026#34;help\u0026#34; for help. postgres=# The warning message is because I have a version 14 psql installed on my machine but I accessed a version 15 server\nThat\u0026rsquo;s it! Now, I can configure my python application to connect to this database container and I will then deploy them using docker-compose\n","permalink":"https://avdlii.xyz/posts/dockerized-postgres/","summary":"As said in my earlier post, my python application has a PostgreSQL backend running on a separate VM. But in order to fully appreciate Docker, I decided to try out PostgreSQL on Docker.\nFirst, let\u0026rsquo;s pull the official PostgreSQL image for alpine\n$ docker pull postgres:15-alpine Then run the image and include the environment variable POSTGRES_PASSWORD for now. We will also add a volume for data persistence.\n$ docker run -itd --rm --name psql \\ \u0026gt; -e POSTGRES_PASSWORD=******** \\ \u0026gt; -v .","title":"Dockerizing My Backend"},{"content":"This afternoon, I have successfully implemented my first Gitlab CI pipeline!\nThe application I have done is made from python flask which I followed thanks to this tutorial. I connected this app to an external PostgreSQL database running on a separate KVM instance in my network. Here\u0026rsquo;s the preview:\nYeah, I know that\u0026rsquo;s not the prettiest to look at. But my priority is to make a functioning application first. I will modify it with CSS later (using my pipeline, of course!)\nOnce I have done this, I turned it into a docker container. Here\u0026rsquo;s the Dockerfile:\nFROM python:3.8-alpine WORKDIR /flask-postgresql COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt COPY . . CMD [ \u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;flask\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;--host=0.0.0.0\u0026#34;] Then I went and tested it locally before pushing it into my DockerHub repo:\ndocker build . -t [docker_username]/python-gitlab-ci:0.1 docker run --name app1 -p 5000:5000 [docker_username]/python-gitlab-ci:0.1 # When confirmed working, it\u0026#39;s time to push it into my dockerhub repo docker push [docker_username]/python-gitlab-ci:0.1 I can then establish my CI pipeline by creating a .gitlab-ci.yml file inside my repo root directory:\nvariables: IMAGE_NAME: [docker_username]/python-gitlab-ci IMAGE_VERSION: 0.1 build_image: image: docker:26.1.4 services: - docker:26.1.4-dind variables: IMAGE_TAG: python-app DOCKER_TLS_CERTDIR: \u0026#34;/certs\u0026#34; before_script: - docker login -u $REGISTRY_USER -p $REGISTRY_PASS script: - docker build . -t $IMAGE_NAME:$IMAGE_TAG-$IMAGE_VERSION - docker push $IMAGE_NAME:$IMAGE_TAG-$IMAGE_VERSION Now everytime I push some changes in my local Gitlab repo, the CI pipeline will trigger and push the new container image to my dockerhub:\nI will use this pipeline as I modify my python-flask application\u0026rsquo;s CSS style.\nHowever, this pipeline is still incomplete. I need to add a testing stage to test my Python code before building the image. Additionally, I require a deployment stage to deploy the container to my Kubernetes cluster.\n","permalink":"https://avdlii.xyz/posts/first-gitlab-ci/","summary":"This afternoon, I have successfully implemented my first Gitlab CI pipeline!\nThe application I have done is made from python flask which I followed thanks to this tutorial. I connected this app to an external PostgreSQL database running on a separate KVM instance in my network. Here\u0026rsquo;s the preview:\nYeah, I know that\u0026rsquo;s not the prettiest to look at. But my priority is to make a functioning application first. I will modify it with CSS later (using my pipeline, of course!","title":"My First Gitlab CI"},{"content":"While tidying up old files on my computer, I came across a PDF document: my first job \u0026rsquo;entrance exam\u0026rsquo; from three years ago. It was structured like an RHCSA exam, presenting real-life scenarios solved entirely through command-line tasks. Seeing this reminded me of my progress.\nInspired by this discovery, I\u0026rsquo;ve decided to revisit these exercises. However, this time, I\u0026rsquo;ll tackle them using Ansible. Here’s a snippet focusing on the user management section, as the full exam consists of 27 items.\n--- - hosts: rh8-an1 tasks: - name: Create groups become: yes group: name: \u0026#34;{{ item }}\u0026#34; loop: - redhat - tech - edb - manager - manage - elk - jboss - admin - name: Create users with groups become: yes user: name: \u0026#34;{{ item.name }}\u0026#34; groups: \u0026#34;{{ item.group }}\u0026#34; state: present append: true loop: - { name: \u0026#39;robin\u0026#39;, group: \u0026#39;redhat,tech\u0026#39; } - { name: \u0026#39;anthony\u0026#39;, group: \u0026#39;edb,rtm\u0026#39; } - { name: \u0026#39;samuel\u0026#39;, group: \u0026#39;manage,\u0026#39; } - { name: \u0026#39;julie\u0026#39;, group: \u0026#39;elk,rtm\u0026#39; } - { name: \u0026#39;wilson\u0026#39;, group: \u0026#39;jboss,rtm\u0026#39; } - { name: \u0026#39;sora\u0026#39;, group: \u0026#39;admin\u0026#39; } - name: Create directories and change group ownership based on respective group become: yes file: path: \u0026#34;/mnt/abc_solutions/{{ item.name }}\u0026#34; group: \u0026#34;{{ item.group }}\u0026#34; state: directory loop: - { name: \u0026#39;redhat\u0026#39;, group: \u0026#39;redhat\u0026#39; } - { name: \u0026#39;edb\u0026#39;, group: \u0026#39;edb\u0026#39; } - { name: \u0026#39;manage\u0026#39;, group: \u0026#39;manage\u0026#39; } - { name: \u0026#39;elk\u0026#39;, group: \u0026#39;elk\u0026#39; } - { name: \u0026#39;jboss\u0026#39;, group: \u0026#39;jboss\u0026#39; } ","permalink":"https://avdlii.xyz/posts/back-to-basics/","summary":"While tidying up old files on my computer, I came across a PDF document: my first job \u0026rsquo;entrance exam\u0026rsquo; from three years ago. It was structured like an RHCSA exam, presenting real-life scenarios solved entirely through command-line tasks. Seeing this reminded me of my progress.\nInspired by this discovery, I\u0026rsquo;ve decided to revisit these exercises. However, this time, I\u0026rsquo;ll tackle them using Ansible. Here’s a snippet focusing on the user management section, as the full exam consists of 27 items.","title":"Back to Basics"},{"content":"As promised, I have created a working bash script for my blog workflow. This makes my entire workflow very efficient and fun!\nThis is my entire bash script:\n#!/bin/bash blogdir=~/blog newpost() { read -p \u0026#34;Enter post filename: \u0026#34; post_filename hugo_post=content/posts/$post_filename.md cd $blogdir hugo new $hugo_post nvim $hugo_post -c :NoNeckPain } publish_post() { read -p \u0026#34;Enter commit message: \u0026#34; commit_message gh_main=~/blog-gh/main/ gh_pages=~/blog-gh/ghpages/ cd $blogdir hugo -d $gh_main hugo -d $gh_pages echo -e \u0026#39;\\n\u0026#39; # commit and push to main branch cd $gh_main echo \u0026#34;-------Main Branch-------\u0026#34; git add . git commit -m \u0026#34;$commit_message\u0026#34; git push echo -e \u0026#39;\\n\u0026#39; # commit and push to gh-pages branch cd $gh_pages echo \u0026#34;-------gh-pages Branch-------\u0026#34; git add . git commit -m \u0026#34;$commit_message\u0026#34; git push echo \u0026#34;-----------------------------\u0026#34; echo -e \u0026#39;\\n\u0026#39; if [[ $? -eq 0 ]] then echo \u0026#34;Blog published.\u0026#34; echo -e \u0026#39;\\n\u0026#39; else echo \u0026#34;Push unsuccessful.\u0026#34; echo -e \u0026#39;\\n\u0026#39; fi } case $1 in new) newpost ;; publish) publish_post ;; \u0026#34;\u0026#34;) # If no parameter is provided, hugo will # serve existing blog iteration at localhost:1313 cd ~/blog/ hugo serve ;; *) echo \u0026#34;Invalid parameter.\u0026#34; ;; esac I had a lot of fun in researching and making this. Now I wonder what else can I automate..\nBy the way, this post is posted using this very script: ","permalink":"https://avdlii.xyz/posts/hugo-bash-script/","summary":"As promised, I have created a working bash script for my blog workflow. This makes my entire workflow very efficient and fun!\nThis is my entire bash script:\n#!/bin/bash blogdir=~/blog newpost() { read -p \u0026#34;Enter post filename: \u0026#34; post_filename hugo_post=content/posts/$post_filename.md cd $blogdir hugo new $hugo_post nvim $hugo_post -c :NoNeckPain } publish_post() { read -p \u0026#34;Enter commit message: \u0026#34; commit_message gh_main=~/blog-gh/main/ gh_pages=~/blog-gh/ghpages/ cd $blogdir hugo -d $gh_main hugo -d $gh_pages echo -e \u0026#39;\\n\u0026#39; # commit and push to main branch cd $gh_main echo \u0026#34;-------Main Branch-------\u0026#34; git add .","title":"Hugo Bash Script"},{"content":"I am contemplating on making a script for automating my posting workflow as there have been repititions that I think can be completely automated.\nBelow I will try my best to illustrate the said workflow using Go ASCII Tool for Markdown. Please forgive my crude attempt:\nh h g u u i g g t p o o u h c l n g t l l e e m o w n l n e e p r o a / s t t e ~ / b l o g g i g t t h g r p - i i u a p p t g b d u a h g l d s g u e i , h e b r s s h c t a o o o b c n p m : r t o m a i p s i n o u t t c n s , h h ~ / g h m b a l i o n g / b g r h a p n a c g h e s ~ / g h b l o g / m a i n As you can see, the flow is quite repetitive. I think It can be very efficient if convert this into a script. So that I can focus my attention entirely on the post itself.\n\u0026ldquo;This is the way.\u0026rdquo;\n","permalink":"https://avdlii.xyz/posts/hugo-script/","summary":"I am contemplating on making a script for automating my posting workflow as there have been repititions that I think can be completely automated.\nBelow I will try my best to illustrate the said workflow using Go ASCII Tool for Markdown. Please forgive my crude attempt:\nh h g u u i g g t p o o u h c l n g t l l e e m o w n l n e e p r o a / s t t e ~ / b l o g g i g t t h g r p - i i u a p p t g b d u a h g l d s g u e i , h e b r s s h c t a o o o b c n p m : r t o m a i p s i n o u t t c n s , h h ~ / g h m b a l i o n g / b g r h a p n a c g h e s ~ / g h b l o g / m a i n As you can see, the flow is quite repetitive.","title":"My Hugo Workflow"},{"content":"When I am first learning about Kubernetes, I am using minikube. But it was all too easy. I want to learn how to set it up from the ground up. What packages are involved, what dependencies, what configuration files should I look for. The answer to this is kubeadm.\nkubeadm is a command line tool that lets me create a Kubernetes cluster from scratch. But before I can usekubeadm, I have to install some packages and configure some os-level stuff.\nI will use Red Hat Enterprise Linux (RHEL) to run my Kubernetes cluster. These are the steps in broad strokes:\nInstall all kubernetes package dependencies dnf install curl gpg ca-certificates iproute-tc -y Create and activate modprobe modules for Kubernetes networking touch /etc/modules-load.d/containerd.conf echo \u0026#34;overlay\u0026#34; \u0026gt; /etc/modules-load.d/containerd.conf echo \u0026#34;br_netfilter\u0026#34; \u0026gt;\u0026gt; /etc/modules-load.d/containerd.conf modprobe overlay modprobe br_netfilter Configure kernel parameters using sysctl for networking echo \u0026#34;net.bridge.bridge-nf-call-iptables = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.d/99-kubernetes-cri.conf echo \u0026#34;net.bridge.bridge-nf-call-ip6tables = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.d/99-kubernetes-cri.conf echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.d/99-kubernetes-cri.conf sysctl --system Download and setup containerd wget https://github.com/containerd/containerd/releases/download/v1.7.16/containerd-1.7.16-linux-amd64.tar.gz -P /tmp/ tar Cxzvf /usr/local /tmp/containerd-1.7.16-linux-amd64.tar.gz wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /etc/systemd/system Download and setup runc wget https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64 -P /tmp/ dnf install -m 755 /tmp/runc.amd64 /usr/local/sbin/runc Download and configure cni wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz -P /tmp/ mkdir -p /opt/cni/bin tar Cxvzf /opt/cni/bin /tmp/cni-plugins-linux-amd64-v1.4.0.tgz Disable swap on OS vi /etc/fstab # comment-out the line with SWAP swapoff -a Setup the Kubernetes repository on the server vi /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/repodata/repomd.xml.key Download Kubernetes packages dnf install kubeadm kubelet kubectl Create a kubernetes control-plane which will act as the main brain of the Kubernetes cluster kubeadm init --pod-network-cidr 192.168.120.0/24 --kubernetes-version 1.30.0 --node-name k8s-control Create kubernetes workers and join them to the kubernetes control-plane server # run on control-plane kubeadm token create --print-join-command # run the output of previous command on worker node/s I know this is far from perfect but its definitely a start.\nWith this, I can further learn how Kubernetes works on a deeper level. One step at a time..\n","permalink":"https://avdlii.xyz/posts/kubeadm/","summary":"When I am first learning about Kubernetes, I am using minikube. But it was all too easy. I want to learn how to set it up from the ground up. What packages are involved, what dependencies, what configuration files should I look for. The answer to this is kubeadm.\nkubeadm is a command line tool that lets me create a Kubernetes cluster from scratch. But before I can usekubeadm, I have to install some packages and configure some os-level stuff.","title":"Setting Up Kubernetes Using Kubeadm"},{"content":"In my previous role, my primary focus was on PostgreSQL databases, specifically in database setup, configuration, and infrastructure rather than on the SQL level. For instance, one of my tasks was establishing streaming replication between a master and two standby database servers.\nI found this work incredibly exciting. I vividly remember the satisfaction of getting streaming replication working during my first week on the job. The ability to configure a system from scratch and see it replicate an entire database from one machine to another was exhilarating!\nHowever, as you\u0026rsquo;re likely aware, working with such systems comes with its challenges.\nWhen the master server/node fails, one of the standby servers needs to take over as the new master. This process involves rebuilding the old master, which initially acts as a standby/slave before being promoted again to master. In my experience, this rebuilding process can be slow, particularly for large databases common in large enterprises. In a business environment where time is money, the extended rebuild times can pose significant challenges.\nDuring my time in this role, a former colleague introduced me to an application running on something called Kubernetes. He demonstrated a feature where intentionally destroying one of the pods resulted in its automatic replacement without manual intervention or ssh-ing inside the pods to revive it.\nComing from a database background, this experience fascinated me. And that was just the tip of the kubernetes iceberg.\nAnd into the rabbit hole I go..\n","permalink":"https://avdlii.xyz/posts/thoughts-on-kubernetes/","summary":"In my previous role, my primary focus was on PostgreSQL databases, specifically in database setup, configuration, and infrastructure rather than on the SQL level. For instance, one of my tasks was establishing streaming replication between a master and two standby database servers.\nI found this work incredibly exciting. I vividly remember the satisfaction of getting streaming replication working during my first week on the job. The ability to configure a system from scratch and see it replicate an entire database from one machine to another was exhilarating!","title":"Thoughts on Kubernetes"},{"content":"For my first post, I thought I would expound on the tech and tools powering this blog.\nLinux Of course, Linux! I will be posting and writing my blog entries using Linux. This blog has also been configured and will be further configured using Linux.\nHugo If you never heard of it, Hugo is an open-source site generator tool that has amazing customizability features. I can build and customize portions of my blog. I can add certain modules here and there with HTML and CSS.\nIn order to successfully launch this blog, I had to learn Hugo. The main configuration, page configuration, filesystem structure, what function every directory have, and so on. What fun.\nGithub Pages I deployed my blog in Github Pages, which is a hosting service provided by Github. It is easy to learn and deploy. Best of all: its free!\nGithub Actions Since I will be writing all my blog entries in my Linux command line, I have to utilize some kind of automation pipeline in order to update my Github Pages blog every time I commit an entry.\nI found this blog that have a Github Actions automation that will do just that. I studied it to familiarize myself with its process and not put it in my repository blindly without knowing how it works.\n","permalink":"https://avdlii.xyz/posts/my-blog/","summary":"For my first post, I thought I would expound on the tech and tools powering this blog.\nLinux Of course, Linux! I will be posting and writing my blog entries using Linux. This blog has also been configured and will be further configured using Linux.\nHugo If you never heard of it, Hugo is an open-source site generator tool that has amazing customizability features. I can build and customize portions of my blog.","title":"My Online Blog"},{"content":"\u003c!DOCTYPE html\u003e My name is Agrielio, but you can call me Gio.\nI am very passionate about Linux and open-source as a general. I enjoy continually learning, experimenting with new concepts, and gaining insights from each experience.\nI believe learning is a lifelong journey, and we should continuously aim to improve in our chosen fields, whether in technology or other pursuits. That's why I consistently strive to exceed my yesterday's self. Even a 1% improvement every day can lead to significant accomplishments in the long run. Operating Systems\nRHEL\nUbuntu\nFedora\nCentOS\nDatabase\nPostgreSQL\nEnterpriseDB\nVersion Control\nGit\nGithub\nGitlab\nAutomation\nBash Script\nAnsible\nTerraform\nContainer and Orchestration\nDocker\nKubernetes\nVirtualization\n(On-prem and Cloud)\nVirtualBox\nKVM\nAWS\nOther Tools I Often Use\nAlacritty\nVim\nNeoVim\nTmux\nHugo\nDotfiles\n","permalink":"https://avdlii.xyz/aboutme/","summary":"\u003c!DOCTYPE html\u003e My name is Agrielio, but you can call me Gio.\nI am very passionate about Linux and open-source as a general. I enjoy continually learning, experimenting with new concepts, and gaining insights from each experience.\nI believe learning is a lifelong journey, and we should continuously aim to improve in our chosen fields, whether in technology or other pursuits. That's why I consistently strive to exceed my yesterday's self.","title":"Hello, friend."}]